{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9c0967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_tictactoe_dassy\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18537c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49ddf5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_tictactoe_dassy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79be49d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Attempted to look up malformed environment ID: b'tictactoe'. (Currently all IDs must be of the form ^(?:[\\w:-]+\\/)?([\\w:.-]+)-v(\\d+)$.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a88a105a3257>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tictactoe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Making new env: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    151\u001b[0m             raise error.Error(\n\u001b[1;32m    152\u001b[0m                 \"Attempted to look up malformed environment ID: {}. (Currently all IDs must be of the form {}.)\".format(\n\u001b[0;32m--> 153\u001b[0;31m                     \u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_id_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m                 )\n\u001b[1;32m    155\u001b[0m             )\n",
      "\u001b[0;31mError\u001b[0m: Attempted to look up malformed environment ID: b'tictactoe'. (Currently all IDs must be of the form ^(?:[\\w:-]+\\/)?([\\w:.-]+)-v(\\d+)$.)"
     ]
    }
   ],
   "source": [
    "env = gym.envs.make('tictactoe-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3d3e153",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qagent(object):\n",
    "    \"\"\"\n",
    "    Implementation of a Q-learning Algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, env, state_size, action_size, learning_parameters, exploration_parameters\n",
    "    ):\n",
    "        \"\"\"\n",
    "        initialize the q-learning agent\n",
    "        Args:\n",
    "          state_size (int): ..\n",
    "          action_size (int): ..\n",
    "          learning_parameters (dict):\n",
    "          exploration_parameters (dict):\n",
    "\n",
    "        \"\"\"\n",
    "        # init the Q-table\n",
    "        self.qtable = np.zeros((state_size, action_size))\n",
    "\n",
    "        # learning parameters\n",
    "        self.learning_rate = learning_parameters[\"learning_rate\"]\n",
    "        self.gamma = learning_parameters[\"gamma\"]\n",
    "\n",
    "        # exploration parameters\n",
    "        self.epsilon = exploration_parameters[\"epsilon\"]\n",
    "        self.max_epsilon = exploration_parameters[\"max_epsilon\"]\n",
    "        self.min_epsilon = exploration_parameters[\"min_epsilon\"]\n",
    "        self.decay_rate = exploration_parameters[\"decay_rate\"]\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "    def update_qtable(self, state, new_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        update the q-table: Q(s,a) = Q(s,a) + lr  * [R(s,a) + gamma * max Q(s',a') - Q (s,a)]\n",
    "        Args:\n",
    "          state (int): current state of the environment\n",
    "          new_state (int): new state of the environment\n",
    "          action (int): current action taken by agent\n",
    "          reward (int): current reward received from env\n",
    "          done (boolean): variable indicating if env is done\n",
    "        Returns:\n",
    "          qtable (array): the qtable containing a value for every state (y-axis) and action (x-axis)\n",
    "        \"\"\"\n",
    "        return self.qtable[state, action] + self.learning_rate * (\n",
    "            reward\n",
    "            + self.gamma * np.max(self.qtable[new_state, :]) * (1 - done)\n",
    "            - self.qtable[state, action]\n",
    "        )\n",
    "\n",
    "    def update_epsilon(self, episode):\n",
    "        \"\"\"\n",
    "        reduce epsilon, exponential decay\n",
    "        Args:\n",
    "          episode (int): number of episode\n",
    "        \"\"\"\n",
    "        self.epsilon = self.min_epsilon + (\n",
    "            self.max_epsilon - self.min_epsilon\n",
    "        ) * np.exp(-self.decay_rate * episode)\n",
    "\n",
    "    def get_action(self, state, action_space):\n",
    "        \"\"\"\n",
    "        select action e-greedy\n",
    "        Args:\n",
    "          state (int): current state of the environment/agent\n",
    "          action_space (array): array with legal actions\n",
    "        Returns:\n",
    "          action (int): action that the agent will take in the next step\n",
    "        \"\"\"\n",
    "        if random.uniform(0, 1) >= self.epsilon:\n",
    "            # exploitation, max value for given state\n",
    "            ranks = self.qtable[state, :].argsort().argsort()\n",
    "            # get ranke of max value (min rank) from the action_space\n",
    "            action = np.where(ranks == np.min(ranks[action_space]))[0][0]\n",
    "\n",
    "        else:\n",
    "            # exploration, random choice\n",
    "            action = np.random.choice(action_space)  # self.env.action_space.sample()\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "876b3d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of legal states: 8953\n",
      "qtable could not be loaded!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "step() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-616ed71b96fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0maction_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction_space\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreshape_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: step() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_tictactoe_dassy\n",
    "\n",
    "\n",
    "\n",
    "state_dict = create_state_dictionary()\n",
    "\n",
    "\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "\n",
    "player1 = 1\n",
    "player2 = 2\n",
    "\n",
    "\n",
    "learning_parameters = {\"learning_rate\": 1.0, \"gamma\": 0.9}\n",
    "\n",
    "exploration_parameters = {\n",
    "    \"epsilon\": 1.0,\n",
    "    \"max_epsilon\": 1.0,\n",
    "    \"min_epsilon\": 0.0,\n",
    "    \"decay_rate\": 0.000001,\n",
    "}\n",
    "\n",
    "# set training parameters\n",
    "episodes = 1000  # 10**6 * 2\n",
    "max_steps = 9\n",
    "\n",
    "# name of the qtable when saved\n",
    "name = \"qtable\"\n",
    "load = True\n",
    "save = True\n",
    "test = True\n",
    "\n",
    "num_test_games = 1\n",
    "\n",
    "player1_reward_array = np.zeros(episodes)\n",
    "player2_reward_array = np.zeros(episodes)\n",
    "\n",
    "# init the q-learning algorithm\n",
    "qagent = Qagent(\n",
    "    env, state_size, action_size, learning_parameters, exploration_parameters\n",
    ")\n",
    "\n",
    "if load:\n",
    "    try:\n",
    "        qagent.qtable = load_qtable(name)\n",
    "        print(\"{}.npy loaded!\".format(name))\n",
    "    except:\n",
    "        print(\"qtable could not be loaded!\")\n",
    "\n",
    "\n",
    "# TODO: Track the actions taken over time while playing,  9*8*7*6*5*4*3*2*1\n",
    "\n",
    "# start the training\n",
    "start_time = time.time()\n",
    "\n",
    "for episode_i in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = state_dict[reshape_state(state)]\n",
    "\n",
    "    action_space = np.arange(9)\n",
    "\n",
    "    # reset the reward of the players\n",
    "    player1_reward = 0\n",
    "    player2_reward = 0\n",
    "\n",
    "    # change start of players, randomly change the order players to start the game\n",
    "    start = np.random.randint(2)  # integer either 0 or 1\n",
    "\n",
    "    for _step in range(start, max_steps + start):\n",
    "        # alternate the moves of the players\n",
    "        if _step % 2 == 0:\n",
    "\n",
    "            # player 1\n",
    "            action = qagent.get_action(state, action_space)\n",
    "\n",
    "            # remove action from the action space\n",
    "            action_space = action_space[action_space != action]\n",
    "\n",
    "            new_state, reward, done, _ = env.step(action, player1)\n",
    "            new_state = state_dict[reshape_state(new_state)]\n",
    "\n",
    "            qagent.qtable[state, action] = qagent.update_qtable(\n",
    "                state, new_state, action, reward, done\n",
    "            )\n",
    "            # new state\n",
    "            state = new_state\n",
    "            player1_reward += reward\n",
    "\n",
    "        else:\n",
    "\n",
    "            # player 2\n",
    "            action = qagent.get_action(state, action_space)\n",
    "            # remove action from the action space\n",
    "            action_space = action_space[action_space != action]\n",
    "            \n",
    "            new_state, reward, done, _ = env.step(action, player2)\n",
    "            new_state = state_dict[reshape_state(new_state)]\n",
    "\n",
    "            qagent.qtable[state, action] = qagent.update_qtable(\n",
    "                state, new_state, action, reward, done\n",
    "            )\n",
    "\n",
    "            # new state\n",
    "            state = new_state\n",
    "            player2_reward += reward\n",
    "\n",
    "        # stopping criterion\n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "    # reduce epsilon for exporation-exploitation tradeoff\n",
    "    qagent.update_epsilon(episode_i)\n",
    "\n",
    "    player1_reward_array[episode_i] = player1_reward\n",
    "    player2_reward_array[episode_i] = player2_reward\n",
    "\n",
    "    if episode_i % 100000 == 0:\n",
    "        print(\"episode: {}, epsilon: {}\".format(episode_i, round(qagent.epsilon, 2)))\n",
    "        print(\n",
    "            \"elapsed time [min]: {}, done [%]: {}\".format(\n",
    "                round((time.time() - start_time) / 60.0, 2), episode_i / episodes * 100\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "if save:\n",
    "    save_qtable(qagent.qtable, name)\n",
    "    qtable = qagent.qtable\n",
    "\n",
    "# test the algorithm with playing against it\n",
    "if test:\n",
    "    test_self_play_learning(env, qtable, max_steps, num_test_games, state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab1364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
