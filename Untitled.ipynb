{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9c0967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_tictactoe_dassy\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e2fa20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('gym_tictactoe_dassy:tictactoe-v0', small=-1, large=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f01bb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3d3e153",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qagent(object):\n",
    "    \"\"\"\n",
    "    Implementation of a Q-learning Algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, env, state_size, action_size, learning_parameters, exploration_parameters\n",
    "    ):\n",
    "        \"\"\"\n",
    "        initialize the q-learning agent\n",
    "        Args:\n",
    "          state_size (int): ..\n",
    "          action_size (int): ..\n",
    "          learning_parameters (dict):\n",
    "          exploration_parameters (dict):\n",
    "\n",
    "        \"\"\"\n",
    "        # init the Q-table\n",
    "        self.qtable = np.zeros((state_size, action_size))\n",
    "\n",
    "        # learning parameters\n",
    "        self.learning_rate = learning_parameters[\"learning_rate\"]\n",
    "        self.gamma = learning_parameters[\"gamma\"]\n",
    "\n",
    "        # exploration parameters\n",
    "        self.epsilon = exploration_parameters[\"epsilon\"]\n",
    "        self.max_epsilon = exploration_parameters[\"max_epsilon\"]\n",
    "        self.min_epsilon = exploration_parameters[\"min_epsilon\"]\n",
    "        self.decay_rate = exploration_parameters[\"decay_rate\"]\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "    def update_qtable(self, state, new_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        update the q-table: Q(s,a) = Q(s,a) + lr  * [R(s,a) + gamma * max Q(s',a') - Q (s,a)]\n",
    "        Args:\n",
    "          state (int): current state of the environment\n",
    "          new_state (int): new state of the environment\n",
    "          action (int): current action taken by agent\n",
    "          reward (int): current reward received from env\n",
    "          done (boolean): variable indicating if env is done\n",
    "        Returns:\n",
    "          qtable (array): the qtable containing a value for every state (y-axis) and action (x-axis)\n",
    "        \"\"\"\n",
    "        return self.qtable[state, action] + self.learning_rate * (\n",
    "            reward\n",
    "            + self.gamma * np.max(self.qtable[new_state, :]) * (1 - done)\n",
    "            - self.qtable[state, action]\n",
    "        )\n",
    "\n",
    "    def update_epsilon(self, episode):\n",
    "        \"\"\"\n",
    "        reduce epsilon, exponential decay\n",
    "        Args:\n",
    "          episode (int): number of episode\n",
    "        \"\"\"\n",
    "        self.epsilon = self.min_epsilon + (\n",
    "            self.max_epsilon - self.min_epsilon\n",
    "        ) * np.exp(-self.decay_rate * episode)\n",
    "\n",
    "    def get_action(self, state, action_space):\n",
    "        \"\"\"\n",
    "        select action e-greedy\n",
    "        Args:\n",
    "          state (int): current state of the environment/agent\n",
    "          action_space (array): array with legal actions\n",
    "        Returns:\n",
    "          action (int): action that the agent will take in the next step\n",
    "        \"\"\"\n",
    "        if random.uniform(0, 1) >= self.epsilon:\n",
    "            # exploitation, max value for given state\n",
    "            ranks = self.qtable[state, :].argsort().argsort()\n",
    "            # get ranke of max value (min rank) from the action_space\n",
    "            action = np.where(ranks == np.min(ranks[action_space]))[0][0]\n",
    "\n",
    "        else:\n",
    "            # exploration, random choice\n",
    "            action = np.random.choice(action_space)  # self.env.action_space.sample()\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42e1e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import product\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_state_dictionary():\n",
    "    \"\"\"\n",
    "    create a dictionary, that encodes the game postions (3x3) into a state number (int)\n",
    "    Returns:\n",
    "      state_dict (dict): key = game position, value = state number\n",
    "    \"\"\"\n",
    "    state_number = 0\n",
    "    state_dict = {}\n",
    "\n",
    "    # create all digit combinations with 0,1,3 for 9 digit number\n",
    "    for game_position in set(product(set(range(3)), repeat=9)):\n",
    "        # count the digits per tuple\n",
    "        count_digits = Counter(game_position)\n",
    "        # remove all board situation, which are not possible\n",
    "        if abs(count_digits[1] - count_digits[2]) <= 1:\n",
    "            state_dict[game_position] = state_number\n",
    "            state_number += 1\n",
    "    print(\"Number of legal states: {}\".format(state_number))\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "def reshape_state(state):\n",
    "    \"\"\"\n",
    "    transfrom the 3x3 board numpy array into a flattend tuple\n",
    "    Args:\n",
    "        state (array): 3x3 numpy array, representing the board postions = state\n",
    "    Returns:\n",
    "        state (tuple): the flattened numy array converted into a tuple\n",
    "    \"\"\"\n",
    "    return tuple(state.reshape(1, -1)[0])\n",
    "\n",
    "\n",
    "def create_plot(player1_reward_array, player2_reward_array):\n",
    "    \"\"\"\n",
    "    plot the rewards of the player 1 and 2 versus the number of training episode in self-play\n",
    "    Args:\n",
    "        player1_reward_array (array): rewards over training episoded player1\n",
    "        player2_reward_array (array): rewards over training episoded player2\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(\"reward over time\")\n",
    "    plt.plot(\n",
    "        range(len(player1_reward_array)), player1_reward_array, label=\"Reward Player 1\"\n",
    "    )\n",
    "    plt.plot(\n",
    "        range(len(player2_reward_array)), player2_reward_array, label=\"Reward Player 2\"\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_qtable(qtable, name=\"qtable\"):\n",
    "    \"\"\"\n",
    "    save the qtable\n",
    "    \"\"\"\n",
    "    np.save(\"{}.npy\".format(name), qtable)\n",
    "    print(\"{}.npy saved!\".format(name))\n",
    "\n",
    "\n",
    "def load_qtable(name=\"qtable\"):\n",
    "    \"\"\"\n",
    "    load the qtable\n",
    "    \"\"\"\n",
    "    return np.load(\"{}.npy\".format(name))\n",
    "\n",
    "\n",
    "def test_self_play_learning(env, qtable, max_steps, num_test_games, state_dict):\n",
    "    \"\"\"\n",
    "    play against the trained Q-Learning agent\n",
    "    Args:\n",
    "        env (class): environment class\n",
    "        qtable (array): numpy array containing the qtable respect. the state-action values\n",
    "        max_steps (int): max steps to take in one episode\n",
    "        num_test_games (int): number of times to play against the trained agent\n",
    "        state_dict (dict): encoding of the state array\n",
    "    \"\"\"\n",
    "\n",
    "    player1 = 1\n",
    "    player2 = 2\n",
    "\n",
    "    for _ in range(num_test_games):\n",
    "        state = env.reset()\n",
    "        state = state_dict[reshape_state(state)]\n",
    "\n",
    "        action_space = np.arange(9)\n",
    "        start = np.random.randint(2)  # 0 or 1\n",
    "\n",
    "        if start == 0:\n",
    "            print(\"Player 1 begins (Human)\")\n",
    "        else:\n",
    "            print(\"Player 2 begins (QAgent)\")\n",
    "        print(\"--\" * 10)\n",
    "        print(\"--\" * 10)\n",
    "\n",
    "        for _step in range(start, max_steps + start):\n",
    "\n",
    "            # alternate the moves of the players\n",
    "            if _step % 2 == 0:\n",
    "                env.render()\n",
    "                print(\"--\" * 10)\n",
    "                print(\"Move Player 1\")\n",
    "                action = np.nan\n",
    "\n",
    "                while action not in action_space:\n",
    "                    action = int(\n",
    "                        input(\"choose an action from {}: \".format(action_space))\n",
    "                    )\n",
    "                    print(\"Action:\", action)\n",
    "                action_space = action_space[action_space != action]\n",
    "\n",
    "                state, reward, done, _ = env.step(action, player1)\n",
    "                state = state_dict[reshape_state(state)]\n",
    "                print(reward)\n",
    "                if done:\n",
    "                    print(\"**\" * 10)\n",
    "                    print(\"player 1 won!\")\n",
    "                    print(\"**\" * 10)\n",
    "                    env.render()\n",
    "                    print(\"\\n\" * 2)\n",
    "                    break\n",
    "            else:\n",
    "                print(\"--\" * 10)\n",
    "                print(\"move player 2\")\n",
    "\n",
    "                array = np.array(qtable[state, :])\n",
    "                order = array.argsort()\n",
    "                ranks = order.argsort()\n",
    "                max_value_rank = np.min(ranks[action_space])\n",
    "                action = np.where(ranks == max_value_rank)[0][0]\n",
    "\n",
    "                print(\"Action:\", action)\n",
    "                action_space = action_space[action_space != action]\n",
    "\n",
    "                state, reward, done, _ = env.step(action, player2)\n",
    "                state = state_dict[reshape_state(state)]\n",
    "                if done:\n",
    "                    print(\"**\" * 10)\n",
    "                    print(\"player 2 won!\")\n",
    "                    print(\"**\" * 10)\n",
    "                    env.render()\n",
    "                    print(\"\\n\" * 2)\n",
    "                    break\n",
    "\n",
    "            # stopping criterion\n",
    "            if not done and _step == max_steps + start - 1:\n",
    "                if reward != env.large - 1:\n",
    "                    print(\"There is no Winner!\")\n",
    "                    print(\"--\" * 10)\n",
    "                    print(\"--\" * 10)\n",
    "                    print(\"\\n\" * 2)\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "876b3d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of legal states: 8953\n",
      "qtable could not be loaded!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "step() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-616ed71b96fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0maction_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction_space\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreshape_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: step() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_tictactoe_dassy\n",
    "\n",
    "\n",
    "\n",
    "state_dict = create_state_dictionary()\n",
    "\n",
    "\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "\n",
    "player1 = 1\n",
    "player2 = 2\n",
    "\n",
    "\n",
    "learning_parameters = {\"learning_rate\": 1.0, \"gamma\": 0.9}\n",
    "\n",
    "exploration_parameters = {\n",
    "    \"epsilon\": 1.0,\n",
    "    \"max_epsilon\": 1.0,\n",
    "    \"min_epsilon\": 0.0,\n",
    "    \"decay_rate\": 0.000001,\n",
    "}\n",
    "\n",
    "# set training parameters\n",
    "episodes = 1000  # 10**6 * 2\n",
    "max_steps = 9\n",
    "\n",
    "# name of the qtable when saved\n",
    "name = \"qtable\"\n",
    "load = True\n",
    "save = True\n",
    "test = True\n",
    "\n",
    "num_test_games = 1\n",
    "\n",
    "player1_reward_array = np.zeros(episodes)\n",
    "player2_reward_array = np.zeros(episodes)\n",
    "\n",
    "# init the q-learning algorithm\n",
    "qagent = Qagent(\n",
    "    env, state_size, action_size, learning_parameters, exploration_parameters\n",
    ")\n",
    "\n",
    "if load:\n",
    "    try:\n",
    "        qagent.qtable = load_qtable(name)\n",
    "        print(\"{}.npy loaded!\".format(name))\n",
    "    except:\n",
    "        print(\"qtable could not be loaded!\")\n",
    "\n",
    "\n",
    "# TODO: Track the actions taken over time while playing,  9*8*7*6*5*4*3*2*1\n",
    "\n",
    "# start the training\n",
    "start_time = time.time()\n",
    "\n",
    "for episode_i in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = state_dict[reshape_state(state)]\n",
    "\n",
    "    action_space = np.arange(9)\n",
    "\n",
    "    # reset the reward of the players\n",
    "    player1_reward = 0\n",
    "    player2_reward = 0\n",
    "\n",
    "    # change start of players, randomly change the order players to start the game\n",
    "    start = np.random.randint(2)  # integer either 0 or 1\n",
    "\n",
    "    for _step in range(start, max_steps + start):\n",
    "        # alternate the moves of the players\n",
    "        if _step % 2 == 0:\n",
    "\n",
    "            # player 1\n",
    "            action = qagent.get_action(state, action_space)\n",
    "\n",
    "            # remove action from the action space\n",
    "            action_space = action_space[action_space != action]\n",
    "\n",
    "            new_state, reward, done, _ = env.step(action, player1)\n",
    "            new_state = state_dict[reshape_state(new_state)]\n",
    "\n",
    "            qagent.qtable[state, action] = qagent.update_qtable(\n",
    "                state, new_state, action, reward, done\n",
    "            )\n",
    "            # new state\n",
    "            state = new_state\n",
    "            player1_reward += reward\n",
    "\n",
    "        else:\n",
    "\n",
    "            # player 2\n",
    "            action = qagent.get_action(state, action_space)\n",
    "            # remove action from the action space\n",
    "            action_space = action_space[action_space != action]\n",
    "            \n",
    "            new_state, reward, done, _ = env.step(action, player2)\n",
    "            new_state = state_dict[reshape_state(new_state)]\n",
    "\n",
    "            qagent.qtable[state, action] = qagent.update_qtable(\n",
    "                state, new_state, action, reward, done\n",
    "            )\n",
    "\n",
    "            # new state\n",
    "            state = new_state\n",
    "            player2_reward += reward\n",
    "\n",
    "        # stopping criterion\n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "    # reduce epsilon for exporation-exploitation tradeoff\n",
    "    qagent.update_epsilon(episode_i)\n",
    "\n",
    "    player1_reward_array[episode_i] = player1_reward\n",
    "    player2_reward_array[episode_i] = player2_reward\n",
    "\n",
    "    if episode_i % 100000 == 0:\n",
    "        print(\"episode: {}, epsilon: {}\".format(episode_i, round(qagent.epsilon, 2)))\n",
    "        print(\n",
    "            \"elapsed time [min]: {}, done [%]: {}\".format(\n",
    "                round((time.time() - start_time) / 60.0, 2), episode_i / episodes * 100\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "if save:\n",
    "    save_qtable(qagent.qtable, name)\n",
    "    qtable = qagent.qtable\n",
    "\n",
    "# test the algorithm with playing against it\n",
    "if test:\n",
    "    test_self_play_learning(env, qtable, max_steps, num_test_games, state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab1364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
