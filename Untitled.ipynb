{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9c0967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_tictactoe_dassy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2fa20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('gym_tictactoe_dassy:tictactoe-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53f01bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e74f9d16",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "render() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d9761596d5d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: render() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b8e81f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(observation, reward, done, info) = env.step([0, 3]) # 0 for player 1 and position 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "289ac85b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "render() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d9761596d5d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: render() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2211202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 0, False, 'normal move player 2')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step([1, 2]) # 1 for player 2 and position 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e8124de",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "render() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d9761596d5d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: render() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d31fdafe",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "attempted to get missing private attribute '_encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-16f251cba979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpreset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             raise AttributeError(\n\u001b[0;32m--> 236\u001b[0;31m                 \u001b[0;34m\"attempted to get missing private attribute '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m             )\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: attempted to get missing private attribute '_encode'"
     ]
    }
   ],
   "source": [
    "preset = [[0, 1, 2], [0, 0, 0], [1, 0, 2]]\n",
    "env.s = env._encode(preset)\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef4d69f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import os.path\n",
    "\n",
    "import gym_tictactoe_dassy\n",
    "\n",
    "\n",
    "def create_Q(env):\n",
    "    \"\"\"\n",
    "    Initializes Q-Table, where:\n",
    "        rows = states\n",
    "        columns = actions\n",
    "        entries = values = sum of accumulated expected reward\n",
    "    Args:\n",
    "        env: The environment to create the Q-table for\n",
    "    Returns:\n",
    "        zero-matrix m x n where:\n",
    "            m = observation space\n",
    "            n = action space\n",
    "    \"\"\"\n",
    "\n",
    "    if from_scratch:\n",
    "        return np.zeros([env.observation_space.n, int(env.action_space.nvec[1])])\n",
    "    else:\n",
    "        try:\n",
    "            print('Loading Q-Table')\n",
    "            return pickle.load(open(\"q_table.p\", \"rb\"))\n",
    "        except IOError:\n",
    "            print('Could not find file. Starting from scratch')\n",
    "            return np.zeros([env.observation_space.n, int(env.action_space.nvec[1])])\n",
    "\n",
    "\n",
    "def get_next_envs(env, turn):\n",
    "    \"\"\"\n",
    "    Get list of possible next environments given a player at turn.\n",
    "    Args:\n",
    "        env: The environment to create the Q-table for\n",
    "        turn: {0, 1}, which player is at turn\n",
    "    Returns:\n",
    "        list of possible gym_tictactoe.envs.tictactoe_env.TictactoeEnv\n",
    "            by looking at all possible moves given the player at turn\n",
    "    \"\"\"\n",
    "\n",
    "    free_moves = env.get_valid_moves()\n",
    "    next_envs = []\n",
    "    for free_move in free_moves:\n",
    "        env_copy = gym.make(\n",
    "            'gym_tictactoe:tictactoe-v0', size=size, num_winning=num_winning)\n",
    "        env_copy.s = env.s\n",
    "        env_copy.step((turn, free_move))\n",
    "        next_envs.append(env_copy)\n",
    "    return (next_envs, free_moves)\n",
    "\n",
    "\n",
    "def minmax_state_value(env, player, turn):\n",
    "    \"\"\"\n",
    "    Calculates the value of a board state given a player's view and a player at turn.\n",
    "    Makes use of minmax algorithm.\n",
    "    Args:\n",
    "        env: The environment\n",
    "        player: {0, 1}, the player's view\n",
    "        turn: {0, 1}, which player is at turn\n",
    "    Returns:\n",
    "        the value of the current state of the environment in the eyes of a player.\n",
    "            int {-1, 0, 1} where:\n",
    "                1 is best value\n",
    "                -1 is worst value\n",
    "    \"\"\"\n",
    "\n",
    "    # end cases\n",
    "    if env._is_win(player+1):\n",
    "        return 1\n",
    "    elif env._is_win(int(not player)+1):\n",
    "        return -1\n",
    "    elif env._is_full():\n",
    "        return 0\n",
    "\n",
    "    # build possible next boards given which player is at turn\n",
    "    (next_envs, _) = get_next_envs(env, turn)\n",
    "\n",
    "    combine_func = max if turn == player else min\n",
    "    return combine_func([minmax_state_value(next_env, player, int(not turn)) for next_env in next_envs])\n",
    "\n",
    "\n",
    "def opponent_minmax(env, player=1):\n",
    "    \"\"\"\n",
    "    Opponent for the agent that chooses the move that will result in the next state with the highest value\n",
    "    given by a minmax algorithm.\n",
    "    Args:\n",
    "        env: The environment\n",
    "        player=1: {0, 1} for which player to decide\n",
    "    Returns:\n",
    "        action to take in form (a, b) where:\n",
    "            a = player = 1\n",
    "            b = field to place stone by index\n",
    "    \"\"\"\n",
    "    (next_envs, free_moves) = get_next_envs(env, player)\n",
    "    return (player, free_moves[np.argmax([minmax_state_value(next_env, player, int(not player)) for next_env in next_envs])])\n",
    "\n",
    "\n",
    "def opponent_random(env, player=1):\n",
    "    \"\"\"\n",
    "    Opponent for the agent that chooses a random position on the board.\n",
    "    Might result in invalid moves which will have no effect\n",
    "    Args:\n",
    "        env: The environment\n",
    "        player=1: {0, 1} for which player to decide\n",
    "    Returns:\n",
    "        action to take in form (a, b) where:\n",
    "            a = player\n",
    "            b = field to place stone by index\n",
    "    \"\"\"\n",
    "\n",
    "    return (player, env.action_space.sample()[1])\n",
    "\n",
    "\n",
    "def opponent_random_better(env, player=1):\n",
    "    \"\"\"\n",
    "    Opponent for the agent that chooses a random free position on the board.\n",
    "    Therefore only chooses a valid action\n",
    "    Args:\n",
    "        env: The environment\n",
    "        player=1: {0, 1} for which player to decide\n",
    "    Returns:\n",
    "        action to take in form (a, b) where:\n",
    "            a = player\n",
    "            b = field to place stone by index\n",
    "    \"\"\"\n",
    "\n",
    "    valid_moves = env.get_valid_moves()\n",
    "    return (player, random.choice(valid_moves))\n",
    "\n",
    "\n",
    "def opponent_human(env, player=1):\n",
    "    \"\"\"\n",
    "    Human opponent. Asks for input via terminal until a valid action is transmitted\n",
    "    Args:\n",
    "        env: The environment\n",
    "        player=1: {0, 1} for which player to decide\n",
    "    Returns:\n",
    "        action to take in form (a, b) where:\n",
    "            a = player\n",
    "            b = field to place stone by index\n",
    "    \"\"\"\n",
    "\n",
    "    action = [player, None]\n",
    "    while action == [player, None] or not env.action_space.contains(action):\n",
    "        print('Pick a move: ', end='')\n",
    "        user_input = input()\n",
    "        action[1] = int(user_input)-1 if user_input.isdigit() else None\n",
    "    return tuple(action)\n",
    "\n",
    "\n",
    "def agent_move(action_space, state, Q, explore, player=0):\n",
    "    \"\"\"\n",
    "    Agent move decision.\n",
    "    Given the state and the values of the Q table, agent chooses action with maximum value.\n",
    "    Chance to also ignore Q-table and to explore new actions. \n",
    "    Args:\n",
    "        action_space: action space of the environment\n",
    "        state: current observed state of the environment\n",
    "        Q: Q-table for exploitation\n",
    "        explore: True/False to tell if able to explore\n",
    "    Returns:\n",
    "        action to take in form (a, b) where:\n",
    "            a = player\n",
    "            b = field to place stone by index\n",
    "    \"\"\"\n",
    "\n",
    "    if explore and random.uniform(0, 1) < epsilon:\n",
    "        return (player, action_space.sample()[1])  # explore action space\n",
    "    else:\n",
    "        return (player, np.argmax(Q[state]))  # exploit learned values\n",
    "\n",
    "\n",
    "def play_one(env, Q, opponent, render=False, update=True, first=True, explore=True):\n",
    "    \"\"\"\n",
    "    Agent plays one match against an opponent.\n",
    "    Args:\n",
    "        env: The environment\n",
    "        Q: Q-table\n",
    "        opponent: function (env) -> action, returns action given an environment\n",
    "        render=False: Whether to display the field after each move\n",
    "        update=True: Whether to update the Q-table\n",
    "        first=True: If agent starts the game\n",
    "        explore=True: If exploration is allowed for agent decision making\n",
    "    Returns:\n",
    "        Tuple of (a, b) where\n",
    "            a = (updated) Q-Table\n",
    "            b = outcome of the move = {None, 'win', 'loss', 'drawn'}\n",
    "    \"\"\"\n",
    "\n",
    "    action_space = env.action_space\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    done = False\n",
    "    agent_moved = False\n",
    "    opponent_moved = False\n",
    "    agent_reward = 0\n",
    "    old_value = None\n",
    "\n",
    "    # Play\n",
    "    while not done:\n",
    "        # Agent moves, skip in first round if second\n",
    "        if first or opponent_moved:\n",
    "            action = agent_move(action_space, state, Q, explore)\n",
    "            (next_state, agent_reward, done, info) = env.step(action)\n",
    "            agent_moved = True\n",
    "            old_value = Q[state, action]\n",
    "\n",
    "            [print('Agent moved:'),\n",
    "                env.render(), print()] if render else None\n",
    "\n",
    "        # Opponent makes a move, but only if not done\n",
    "        if not done:\n",
    "            opponent_action = opponent(env)\n",
    "            (next_state, opponent_reward, done,\n",
    "                info) = env.step(opponent_action)\n",
    "            opponent_moved = True\n",
    "\n",
    "            [env.render(), print()] if render else None\n",
    "        else:\n",
    "            opponent_reward = 0\n",
    "\n",
    "        # update Q Table but only after opponent has moved\n",
    "        if update and agent_moved:\n",
    "            agent_reward -= opponent_reward\n",
    "            next_value = np.max(Q[next_state])\n",
    "            temp_diff = agent_reward + gamma * next_value - old_value\n",
    "            Q[state, action] = old_value + alpha * temp_diff\n",
    "        state = next_state\n",
    "\n",
    "    # Game finished, get outcome for agent\n",
    "    outcome = None\n",
    "    if env._is_win(1):\n",
    "        outcome = 'win'\n",
    "    elif env._is_win(2):\n",
    "        outcome = 'loss'\n",
    "    else:\n",
    "        outcome = 'drawn'\n",
    "\n",
    "    return (Q, outcome)\n",
    "\n",
    "\n",
    "def train(epochs, env, Q, opponents):\n",
    "    \"\"\"\n",
    "    Train the agent\n",
    "    Args:\n",
    "        epochs: Number of epochs to train\n",
    "        env: The environment\n",
    "        Q: Q-table\n",
    "        opponents: list of opponents as functions (env) -> action to play against\n",
    "    Returns:\n",
    "        updated Q-table\n",
    "    \"\"\"\n",
    "\n",
    "    is_first = True\n",
    "    for i in range(epochs):\n",
    "        (Q, _) = play_one(env, Q, random.choice(opponents), first=is_first)\n",
    "        is_first = not is_first\n",
    "\n",
    "    return Q\n",
    "\n",
    "\n",
    "def test(epochs, env, Q, opponent, render=False):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of the agent by playing against one opponent.\n",
    "    No exploration, no updates to Q-table.\n",
    "    Args:\n",
    "        epochs: Number of epochs to train\n",
    "        env: The environment\n",
    "        Q: Q-table\n",
    "        opponent: opponent as function (env) -> action to play against\n",
    "        render=False: If board shall be rendered and game outcome printed\n",
    "    Returns:\n",
    "        Tuple (wins, losses, drawns)\n",
    "    \"\"\"\n",
    "\n",
    "    outcome_list = [None for i in range(epochs)]\n",
    "    is_first = True\n",
    "    for i in range(epochs):\n",
    "        (_, outcome_agent) = play_one(env, Q, opponent, render=render,\n",
    "                                      first=is_first, update=False, explore=False)\n",
    "\n",
    "        if render:\n",
    "            if outcome_agent == 'win':\n",
    "                print('You lost')\n",
    "            elif outcome_agent == 'loss':\n",
    "                print('You won')\n",
    "            else:\n",
    "                print('Drawn')\n",
    "\n",
    "        outcome_list[i] = outcome_agent\n",
    "        is_first = not is_first\n",
    "\n",
    "    wins = sum(1 for i in outcome_list if i == 'win')\n",
    "    losses = sum(1 for i in outcome_list if i == 'loss')\n",
    "    drawns = sum(1 for i in outcome_list if i == 'drawn')\n",
    "\n",
    "    return wins, losses, drawns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a5c9f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Q-Table\n",
      "Could not find file. Starting from scratch\n",
      "Learning for 500000 epochs\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "attempted to get missing private attribute '_is_win'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-24f7ebf9f828>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-24f7ebf9f828>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Play against a random player and learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Learning for {epochs} epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mopponent_random\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopponent_random_better\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Finished!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saving Q-Table'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-dfc5fc271cf7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, env, Q, opponents)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0mis_first\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopponents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mis_first\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-dfc5fc271cf7>\u001b[0m in \u001b[0;36mplay_one\u001b[0;34m(env, Q, opponent, render, update, first, explore)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;31m# Game finished, get outcome for agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0moutcome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_win\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0moutcome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'win'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_win\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             raise AttributeError(\n\u001b[0;32m--> 236\u001b[0;31m                 \u001b[0;34m\"attempted to get missing private attribute '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m             )\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: attempted to get missing private attribute '_is_win'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function:\n",
    "        1. Create environment\n",
    "        2. Preload Q-table if present, otherwise create from scratch\n",
    "        3. Train for n epochs playing against random opponent\n",
    "        4. Test for n/10 epochs and show performance\n",
    "        5. Play against human\n",
    "    Args:\n",
    "        -\n",
    "    Returns:\n",
    "        -\n",
    "    \"\"\"\n",
    "\n",
    "    # create environment\n",
    "    env = gym.make('gym_tictactoe_dassy:tictactoe-v0',\n",
    "                   size=size, num_winning=num_winning)\n",
    "\n",
    "    # create empty Q-Table or preload\n",
    "    Q = create_Q(env)\n",
    "\n",
    "    # Play against a random player and learn\n",
    "    print(f'Learning for {epochs} epochs')\n",
    "    Q = train(epochs, env, Q, [opponent_random, opponent_random_better])\n",
    "    print(f'Finished!')\n",
    "    print('Saving Q-Table')\n",
    "    pickle.dump(Q, open(\"q_table.p\", \"wb\"))\n",
    "\n",
    "    # Test\n",
    "    print(f'Testing for {int(epochs/10)} epochs')\n",
    "    wins, losses, drawns = test(int(epochs/10), env, Q, opponent_random_better)\n",
    "    print(f'Wins: {wins}, Losses: {losses}, Drawns: {drawns}')\n",
    "\n",
    "    # Play against human player\n",
    "    print(f'Playing 4 games against human player')\n",
    "    losses, wins, drawns = test(4, env, Q, opponent_human, render=True)\n",
    "    print(f'Wins: {wins}, Losses: {losses}, Drawns: {drawns}')\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "epsilon = 0.1  # exploration rate\n",
    "alpha = 0.1  # learning rate\n",
    "gamma = 0.8  # disount factor\n",
    "epochs = 500000  # number of games played while training\n",
    "\n",
    "# other\n",
    "from_scratch = False\n",
    "\n",
    "# Board settings\n",
    "size = 3\n",
    "num_winning = 3\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d3e153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
